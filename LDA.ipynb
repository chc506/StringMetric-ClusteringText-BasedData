{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from stop_words import get_stop_words  \n",
    "from nltk.stem.porter import PorterStemmer  \n",
    "from gensim import corpora, models  \n",
    "import gensim  \n",
    "  \n",
    "\n",
    "\n",
    "doc_set=[]\n",
    "fd= open( \"data.txt\", \"r\" )    \n",
    "for line in fd.readlines():      \n",
    "    doc_set.append(line)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')  \n",
    "\n",
    "# create English stop words list  \n",
    "en_stop = get_stop_words('en') \n",
    "\n",
    "  \n",
    "# Create p_stemmer of class PorterStemmer  \n",
    "p_stemmer = PorterStemmer()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list for tokenized documents in loop  \n",
    "texts = []  \n",
    "  \n",
    "# loop through document list  \n",
    "for i in doc_set:  \n",
    "      \n",
    "    \n",
    "    # clean and tokenize document string  \n",
    "    raw = i.lower()  \n",
    "    tokens = tokenizer.tokenize(raw)  \n",
    "  \n",
    "    \n",
    "    # remove stop words from tokens  \n",
    "    stopped_tokens = [temp for temp in tokens if not temp in en_stop]  \n",
    "      \n",
    "    \n",
    "    # stem tokens  \n",
    "    stemmed_tokens = [p_stemmer.stem(temp) for temp in stopped_tokens]  \n",
    "      \n",
    "    \n",
    "    # add tokens to list  \n",
    "    texts.append(stemmed_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.104*\"tip\" + 0.094*\"4\" + 0.071*\"twin\" + 0.065*\"pack\" + 0.061*\"fine\" + 0.056*\"perman\" + 0.043*\"sharpi\" + 0.042*\"marker\" + 0.041*\"black\" + 0.018*\"ultra\"'), (1, '0.100*\"highlight\" + 0.090*\"style\" + 0.090*\"accent\" + 0.083*\"sharpi\" + 0.067*\"color\" + 0.056*\"fluoresc\" + 0.043*\"yellow\" + 0.041*\"officemax\" + 0.036*\"set\" + 0.031*\"pocket\"'), (2, '0.124*\"r\" + 0.074*\"sharpi\" + 0.062*\"offic\" + 0.062*\"depot\" + 0.059*\"sanford\" + 0.058*\"marker\" + 0.052*\"perman\" + 0.044*\"pack\" + 0.044*\"12\" + 0.041*\"fine\"'), (3, '0.080*\"color\" + 0.066*\"sharpi\" + 0.055*\"set\" + 0.053*\"perman\" + 0.042*\"chisel\" + 0.041*\"tip\" + 0.040*\"black\" + 0.039*\"marker\" + 0.033*\"fine\" + 0.033*\"pen\"'), (4, '0.119*\"fine\" + 0.103*\"marker\" + 0.098*\"sharpi\" + 0.076*\"perman\" + 0.061*\"color\" + 0.054*\"tip\" + 0.050*\"officemax\" + 0.045*\"black\" + 0.040*\"point\" + 0.030*\"ultra\"')]\n"
     ]
    }
   ],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary  \n",
    "dictionary = corpora.Dictionary(texts)  \n",
    "\n",
    "#print(dictionary.token2id) \n",
    "\n",
    "\n",
    "      \n",
    "# convert tokenized documents into a document-term matrix  \n",
    "corpus = [dictionary.doc2bow(text) for text in texts]  \n",
    "\n",
    "\n",
    "  \n",
    "# generate LDA model  \n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=20)  \n",
    "\n",
    "print(ldamodel.print_topics(20))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# project original corpus(plain bag-of-words count vectors) into LDA-topic distributions\n",
    "corpus_lda = ldamodel[corpus]\n",
    "\n",
    "\n",
    "corpus_lda_list=[]\n",
    "for doc in corpus_lda:\n",
    "    corpus_lda_list.append(doc)\n",
    "  \n",
    "\n",
    "# classify according to LDA-topic distributions\n",
    "corpus_lda_label=[];\n",
    "\n",
    "for index in corpus_lda_list:\n",
    "    label=-1;\n",
    "    max_prob=-1;\n",
    "    for topic in index:\n",
    "        if topic[1]>max_prob:\n",
    "            max_prob=topic[1];\n",
    "            label=topic[0];\n",
    "    corpus_lda_label.append(label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
